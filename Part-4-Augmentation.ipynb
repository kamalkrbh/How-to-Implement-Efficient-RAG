{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a17386",
   "metadata": {},
   "source": [
    "## Setup: Prerequisites from Previous Parts\n",
    "\n",
    "We'll reuse some components from Part 2 (Data Indexing) and Part 3 (Retrieval Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa29413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sample data and required libraries\n",
    "from sample_data import SAMPLE_TEXT\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "import torch\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deecca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic chunking function (from Part 2)\n",
    "def chunk_by_semantic_similarity(text: str, similarity_threshold: float = 0.5, overlap_sentences: int = 2, min_chunk_size: int = 2) -> list:\n",
    "    \"\"\"Semantic chunking based on sentence similarity using TF-IDF vectors\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= min_chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    try:\n",
    "        sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "    except ValueError:\n",
    "        return [text]\n",
    "    \n",
    "    similarities = [cosine_similarity(sentence_vectors[i:i+1], sentence_vectors[i+1:i+2])[0][0] \n",
    "                   for i in range(len(sentences) - 1)]\n",
    "    \n",
    "    chunk_boundaries = [0]\n",
    "    current_chunk_size = 1\n",
    "    \n",
    "    for i, sim in enumerate(similarities):\n",
    "        if sim < similarity_threshold and current_chunk_size >= min_chunk_size:\n",
    "            chunk_boundaries.append(i + 1)\n",
    "            current_chunk_size = 1\n",
    "        else:\n",
    "            current_chunk_size += 1\n",
    "    \n",
    "    if chunk_boundaries[-1] != len(sentences):\n",
    "        chunk_boundaries.append(len(sentences))\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(len(chunk_boundaries) - 1):\n",
    "        start_idx = chunk_boundaries[i]\n",
    "        end_idx = chunk_boundaries[i + 1]\n",
    "        \n",
    "        if i > 0 and overlap_sentences > 0:\n",
    "            overlap_start = max(0, start_idx - overlap_sentences)\n",
    "            chunk_sentences = sentences[overlap_start:end_idx]\n",
    "        else:\n",
    "            chunk_sentences = sentences[start_idx:end_idx]\n",
    "        \n",
    "        chunk = \" \".join(chunk_sentences)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create semantic chunks\n",
    "chunks = chunk_by_semantic_similarity(SAMPLE_TEXT, similarity_threshold=0.15, overlap_sentences=2)\n",
    "print(f\"‚úÖ Created {len(chunks)} semantic chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vector store (from Part 2)\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Embedding model loaded!\\n\")\n",
    "\n",
    "print(f\"üîÑ Generating embeddings for {len(chunks)} chunks...\")\n",
    "embeddings = embedding_model.encode(chunks, show_progress_bar=False)\n",
    "print(f\"‚úÖ Generated {len(embeddings)} embeddings\\n\")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"augmentation_demo_chunks\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# Store chunks with metadata\n",
    "collection.add(\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks))],\n",
    "    embeddings=embeddings.tolist(),\n",
    "    documents=chunks,\n",
    "    metadatas=[{\n",
    "        \"chunk_index\": i, \n",
    "        \"length\": len(chunk),\n",
    "        \"source\": \"Remote Work Policy\",\n",
    "        \"topic\": chunk.split('\\n')[0] if '\\n' in chunk else \"General\"\n",
    "    } for i, chunk in enumerate(chunks)]\n",
    ")\n",
    "print(f\"‚úÖ Vector store ready with {collection.count()} chunks\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ae91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid retrieval function (from Part 3)\n",
    "def hybrid_retrieval(query: str, top_k: int = 3, alpha: float = 0.7):\n",
    "    \"\"\"Hybrid retrieval combining BM25 + vector embeddings\"\"\"\n",
    "    # Dense retrieval\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    vector_results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    vector_scores_dict = {}\n",
    "    for chunk_id, distance in zip(vector_results['ids'][0], vector_results['distances'][0]):\n",
    "        chunk_idx = int(chunk_id.split('_')[1])\n",
    "        vector_scores_dict[chunk_idx] = 1 - distance\n",
    "    \n",
    "    # BM25\n",
    "    tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
    "    query_tokens = query.lower().split()\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "    \n",
    "    bm25_scores_dict = {}\n",
    "    for chunk_idx in vector_scores_dict.keys():\n",
    "        bm25_scores_dict[chunk_idx] = bm25.get_scores(query_tokens)[chunk_idx]\n",
    "    \n",
    "    # Normalize BM25\n",
    "    bm25_scores_list = list(bm25_scores_dict.values())\n",
    "    bm25_min = min(bm25_scores_list)\n",
    "    bm25_max = max(bm25_scores_list)\n",
    "    bm25_scores_normalized_dict = {\n",
    "        chunk_idx: (score - bm25_min) / (bm25_max - bm25_min + 1e-10)\n",
    "        for chunk_idx, score in bm25_scores_dict.items()\n",
    "    }\n",
    "    \n",
    "    # Combine scores\n",
    "    hybrid_scores_dict = {\n",
    "        chunk_idx: alpha * vector_scores_dict[chunk_idx] + (1 - alpha) * bm25_scores_normalized_dict[chunk_idx]\n",
    "        for chunk_idx in vector_scores_dict.keys()\n",
    "    }\n",
    "    \n",
    "    sorted_results = sorted(hybrid_scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for chunk_idx, hybrid_score in sorted_results:\n",
    "        # Get metadata from collection\n",
    "        chunk_metadata = collection.get(ids=[f\"chunk_{chunk_idx}\"])['metadatas'][0]\n",
    "        results.append({\n",
    "            'chunk_idx': chunk_idx,\n",
    "            'hybrid_score': hybrid_score,\n",
    "            'vector_score': vector_scores_dict[chunk_idx],\n",
    "            'bm25_score': bm25_scores_normalized_dict[chunk_idx],\n",
    "            'content': chunks[chunk_idx],\n",
    "            'metadata': chunk_metadata\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Hybrid retrieval function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM client\n",
    "groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "print(\"‚úÖ LLM client initialized successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7e268",
   "metadata": {},
   "source": [
    "## 1. Prompt Template Design\n",
    "\n",
    "The prompt template is the blueprint for how we combine retrieved context with the user's question. A well-designed prompt template:\n",
    "\n",
    "- **Provides clear instructions** to the LLM about its role\n",
    "- **Structures the context** in an easy-to-parse format\n",
    "- **Guides response format** (e.g., \"cite sources\", \"be concise\")\n",
    "- **Sets constraints** (e.g., \"only use provided context\")\n",
    "\n",
    "### Common Prompt Template Patterns:\n",
    "\n",
    "1. **Basic QA Template**: Simple question + context format\n",
    "2. **Instructional Template**: Detailed instructions with role definition\n",
    "3. **Citation-focused Template**: Emphasizes source attribution\n",
    "4. **Chain-of-Thought Template**: Encourages step-by-step reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d569a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define various prompt templates\n",
    "\n",
    "# Template 1: Basic QA\n",
    "BASIC_QA_TEMPLATE = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Template 2: Instructional with Role\n",
    "INSTRUCTIONAL_TEMPLATE = \"\"\"You are a helpful assistant answering questions about remote work policies.\n",
    "\n",
    "Instructions:\n",
    "- Use ONLY the information provided in the context below\n",
    "- If the answer is not in the context, say \"I don't have enough information to answer this question\"\n",
    "- Be concise and accurate\n",
    "- Cite the relevant section when possible\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Template 3: Citation-Focused\n",
    "CITATION_TEMPLATE = \"\"\"You are a policy assistant that provides accurate answers with proper citations.\n",
    "\n",
    "Below is relevant information from our company's Remote Work Policy:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "- Answer the question using the information above\n",
    "- Include citations in the format [Source: chunk_X] after each fact\n",
    "- If information is not available, clearly state this\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Template 4: Chain-of-Thought\n",
    "CHAIN_OF_THOUGHT_TEMPLATE = \"\"\"You are an analytical assistant that explains your reasoning.\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. First, identify the relevant information from the context\n",
    "2. Explain your reasoning step-by-step\n",
    "3. Provide a clear, final answer\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "print(\"‚úÖ Prompt templates defined!\")\n",
    "print(f\"\\nAvailable templates:\")\n",
    "print(\"1. BASIC_QA_TEMPLATE\")\n",
    "print(\"2. INSTRUCTIONAL_TEMPLATE\")\n",
    "print(\"3. CITATION_TEMPLATE\")\n",
    "print(\"4. CHAIN_OF_THOUGHT_TEMPLATE\")\n",
    "\n",
    "# Demo: Show what each template looks like with sample data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEMO: Prompt Template Examples\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Sample data for demonstration\n",
    "sample_context = \"\"\"[Chunk 1]\n",
    "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download and 5 Mbps upload.\n",
    "\n",
    "[Chunk 2]\n",
    "All devices must have up-to-date antivirus software and firewalls enabled. The IT department provides technical support.\"\"\"\n",
    "\n",
    "sample_query = \"What are the internet requirements for remote work?\"\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Template 1: BASIC_QA_TEMPLATE\")\n",
    "print(\"-\" * 80)\n",
    "example1 = BASIC_QA_TEMPLATE.format(context=sample_context, query=sample_query)\n",
    "print(example1)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Template 2: INSTRUCTIONAL_TEMPLATE\")\n",
    "print(\"-\" * 80)\n",
    "example2 = INSTRUCTIONAL_TEMPLATE.format(context=sample_context, query=sample_query)\n",
    "print(example2)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Template 3: CITATION_TEMPLATE\")\n",
    "print(\"-\" * 80)\n",
    "example3 = CITATION_TEMPLATE.format(context=sample_context, query=sample_query)\n",
    "print(example3)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Template 4: CHAIN_OF_THOUGHT_TEMPLATE\")\n",
    "print(\"-\" * 80)\n",
    "example4 = CHAIN_OF_THOUGHT_TEMPLATE.format(context=sample_context, query=sample_query)\n",
    "print(example4)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üí° Notice how each template structures the same information differently!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0104f9",
   "metadata": {},
   "source": [
    "## 2. Context Integration Strategies\n",
    "\n",
    "After retrieving relevant chunks, we need to decide **how** to integrate them into the prompt. Different strategies work better for different scenarios:\n",
    "\n",
    "### Strategy 1: **Simple Concatenation**\n",
    "- Join all chunks with separators\n",
    "- Fast and preserves all details\n",
    "- Risk: May exceed token limits with many chunks\n",
    "\n",
    "### Strategy 2: **Numbered/Labeled Context**\n",
    "- Add identifiers to each chunk (e.g., [Chunk 1], [Section A])\n",
    "- Enables easy citation and reference\n",
    "- Better for attribution and debugging\n",
    "\n",
    "### Strategy 3: **Metadata-Enriched Context**\n",
    "- Include metadata (source, date, topic) with each chunk\n",
    "- Provides additional context to the LLM\n",
    "- Useful for multi-document retrieval\n",
    "\n",
    "### Strategy 4: **Summarized Context**\n",
    "- Use LLM to summarize retrieved chunks before augmentation\n",
    "- Reduces token usage\n",
    "- Risk: May lose important details\n",
    "\n",
    "### Strategy 5: **Hierarchical Context**\n",
    "- Organize chunks by relevance or topic\n",
    "- Present most relevant first\n",
    "- Helps LLM prioritize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Integration Strategy Implementations\n",
    "\n",
    "\n",
    "def simple_concatenation(retrieved_results):\n",
    "    \"\"\"Strategy 1: Simple concatenation with separators\"\"\"\n",
    "    # Joins all chunks with separators - fast, preserves all details, but no structure\n",
    "    context = \"\\n\\n---\\n\\n\".join([result[\"content\"] for result in retrieved_results])\n",
    "    return context\n",
    "\n",
    "\n",
    "def numbered_context(retrieved_results):\n",
    "    \"\"\"Strategy 2: Add chunk numbers for citation\"\"\"\n",
    "    # Adds [Chunk X] labels to enable easy citation and reference tracking\n",
    "    context_parts = []\n",
    "    for i, result in enumerate(retrieved_results, 1):\n",
    "        context_parts.append(f\"[Chunk {result['chunk_idx']}]\\n{result['content']}\")\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def metadata_enriched_context(retrieved_results):\n",
    "    \"\"\"Strategy 3: Include metadata with each chunk\"\"\"\n",
    "    # Adds source, topic, and chunk ID headers - provides rich context to LLM\n",
    "    context_parts = []\n",
    "    for result in retrieved_results:\n",
    "        metadata = result[\"metadata\"]\n",
    "        header = f\"[Source: {metadata['source']} | Topic: {metadata['topic']} | Chunk {result['chunk_idx']}]\"\n",
    "        context_parts.append(f\"{header}\\n{result['content']}\")\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def hierarchical_context(retrieved_results):\n",
    "    \"\"\"Strategy 5: Organize by relevance score\"\"\"\n",
    "    # Shows relevance scores with each chunk - helps LLM prioritize information\n",
    "    context_parts = []\n",
    "    for i, result in enumerate(retrieved_results, 1):\n",
    "        relevance = (\n",
    "            \"High\"\n",
    "            if result[\"hybrid_score\"] > 0.7\n",
    "            else \"Medium\" if result[\"hybrid_score\"] > 0.5 else \"Low\"\n",
    "        )\n",
    "        header = f\"[Relevance: {relevance} | Score: {result['hybrid_score']:.3f}]\"\n",
    "        context_parts.append(f\"{header}\\n{result['content']}\")\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def summarized_context(retrieved_results, groq_client):\n",
    "    \"\"\"Strategy 4: Summarize chunks before augmentation\"\"\"\n",
    "    # Uses LLM to compress context - reduces tokens but may lose details\n",
    "    # First concatenate all chunks\n",
    "    full_context = \"\\n\\n\".join([result[\"content\"] for result in retrieved_results])\n",
    "\n",
    "    # Ask LLM to summarize\n",
    "    summary_prompt = f\"\"\"Summarize the following text concisely, preserving key facts and details:\n",
    "\n",
    "{full_context}\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Context integration strategies implemented!\")\n",
    "print(f\"\\nAvailable strategies:\")\n",
    "print(\"1. simple_concatenation()\")\n",
    "print(\"2. numbered_context()\")\n",
    "print(\"3. metadata_enriched_context()\")\n",
    "print(\"4. summarized_context()\")\n",
    "print(\"5. hierarchical_context()\")\n",
    "\n",
    "# Demo: Test each strategy with a sample retrieval\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEMO: Context Integration Strategies\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "sample_query = \"What are the internet speed requirements?\"\n",
    "print(f\"üîç Query: '{sample_query}'\")\n",
    "print(f\"üì• Retrieving chunks...\\n\")\n",
    "\n",
    "sample_results = hybrid_retrieval(sample_query, top_k=2)\n",
    "\n",
    "# Show raw retrieved results BEFORE formatting\n",
    "print(\"=\" * 80)\n",
    "print(\"RAW RETRIEVED RESULTS (Before Context Integration)\")\n",
    "print(\"=\" * 80)\n",
    "for i, result in enumerate(sample_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Chunk ID: {result['chunk_idx']}\")\n",
    "    print(f\"  Hybrid Score: {result['hybrid_score']:.4f}\")\n",
    "    print(f\"  Vector Score: {result['vector_score']:.4f}\")\n",
    "    print(f\"  BM25 Score: {result['bm25_score']:.4f}\")\n",
    "    print(f\"  Metadata: {result['metadata']}\")\n",
    "    print(f\"  Full Content:\")\n",
    "    print(f\"  {'-' * 76}\")\n",
    "    print(f\"  {result['content']}\")\n",
    "    print(f\"  {'-' * 76}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° Joins chunks with separators - fast, preserves all details\")\n",
    "print(\"-\" * 80)\n",
    "context1 = simple_concatenation(sample_results)\n",
    "print(context1)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Strategy 2: Numbered Context\")\n",
    "print(\"üí° Adds [Chunk X] labels for easy citation and reference tracking\")\n",
    "print(\"-\" * 80)\n",
    "context2 = numbered_context(sample_results)\n",
    "print(context2)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Strategy 3: Metadata-Enriched Context\")\n",
    "print(\"üí° Includes source, topic, and chunk info - provides rich context to LLM\")\n",
    "print(\"-\" * 80)\n",
    "context3 = metadata_enriched_context(sample_results)\n",
    "print(context3)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Strategy 4: Hierarchical Context\")\n",
    "print(\"üí° Shows relevance scores - helps LLM prioritize important information\")\n",
    "print(\"-\" * 80)\n",
    "context4 = hierarchical_context(sample_results)\n",
    "print(context4)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üí° Each strategy formats the same retrieved chunks differently!\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448691ed",
   "metadata": {},
   "source": [
    "## 3. Complete Augmentation Pipeline\n",
    "\n",
    "Now let's build a complete augmentation function that combines:\n",
    "1. Retrieval (from Part 3)\n",
    "2. Context integration strategy\n",
    "3. Prompt template selection\n",
    "4. Final prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2cbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_query(\n",
    "    query: str,\n",
    "    top_k: int = 3,\n",
    "    context_strategy: str = \"numbered\",\n",
    "    template_type: str = \"instructional\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete augmentation pipeline:\n",
    "    1. Retrieve relevant chunks\n",
    "    2. Apply context integration strategy\n",
    "    3. Build final prompt using template\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of chunks to retrieve\n",
    "        context_strategy: 'simple', 'numbered', 'metadata', 'hierarchical', 'summarized'\n",
    "        template_type: 'basic', 'instructional', 'citation', 'chain_of_thought'\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'prompt', 'retrieved_results', 'context'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    print(f\"üì• Retrieving top {top_k} chunks...\")\n",
    "    retrieved_results = hybrid_retrieval(query, top_k=top_k)\n",
    "    print(f\"‚úÖ Retrieved {len(retrieved_results)} chunks\\n\")\n",
    "    \n",
    "    # Step 2: Apply context integration strategy\n",
    "    print(f\"üîß Applying context strategy: {context_strategy}\")\n",
    "    if context_strategy == \"simple\":\n",
    "        context = simple_concatenation(retrieved_results)\n",
    "    elif context_strategy == \"numbered\":\n",
    "        context = numbered_context(retrieved_results)\n",
    "    elif context_strategy == \"metadata\":\n",
    "        context = metadata_enriched_context(retrieved_results)\n",
    "    elif context_strategy == \"hierarchical\":\n",
    "        context = hierarchical_context(retrieved_results)\n",
    "    elif context_strategy == \"summarized\":\n",
    "        context = summarized_context(retrieved_results, groq_client)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown context strategy: {context_strategy}\")\n",
    "    \n",
    "    print(f\"‚úÖ Context prepared ({len(context)} chars)\\n\")\n",
    "    \n",
    "    # Step 3: Select and apply prompt template\n",
    "    print(f\"üìù Applying template: {template_type}\")\n",
    "    if template_type == \"basic\":\n",
    "        template = BASIC_QA_TEMPLATE\n",
    "    elif template_type == \"instructional\":\n",
    "        template = INSTRUCTIONAL_TEMPLATE\n",
    "    elif template_type == \"citation\":\n",
    "        template = CITATION_TEMPLATE\n",
    "    elif template_type == \"chain_of_thought\":\n",
    "        template = CHAIN_OF_THOUGHT_TEMPLATE\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown template type: {template_type}\")\n",
    "    \n",
    "    # Build final prompt\n",
    "    final_prompt = template.format(context=context, query=query)\n",
    "    print(f\"‚úÖ Final prompt ready ({len(final_prompt)} chars)\\n\")\n",
    "    \n",
    "    return {\n",
    "        'prompt': final_prompt,\n",
    "        'retrieved_results': retrieved_results,\n",
    "        'context': context,\n",
    "        'query': query\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Complete augmentation pipeline ready!\")\n",
    "\n",
    "# Demo: Test the pipeline with a real query\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AUGMENTATION DEMO: End-to-End Pipeline with Citation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_query = \"What are the internet speed requirements for remote work?\"\n",
    "print(f\"\\nüîç User Query: '{test_query}'\\n\")\n",
    "\n",
    "print(\"üí° Why Citations Matter:\")\n",
    "print(\"   - Builds user trust and credibility\")\n",
    "print(\"   - Allows verification of information\")\n",
    "print(\"   - Reduces hallucinations by grounding answers in sources\")\n",
    "print(\"   - Enables traceability for compliance and auditing\")\n",
    "print(\"   - Helps users explore related information\\n\")\n",
    "\n",
    "# Use numbered context + citation template (best for attribution)\n",
    "augmented_result = augment_query(\n",
    "    query=test_query,\n",
    "    top_k=3,\n",
    "    context_strategy=\"numbered\",\n",
    "    template_type=\"citation\"\n",
    ")\n",
    "\n",
    "print(\"\\nüìã Final Augmented Prompt:\")\n",
    "print(\"=\" * 80)\n",
    "print(augmented_result['prompt'])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a596cf2",
   "metadata": {},
   "source": [
    "## 4. Advanced: Context Window Management\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Note: Context Window vs. LLM Memory\n",
    "\n",
    "**Context Window** = Maximum tokens the LLM can process in one request (what we're managing here)\n",
    "\n",
    "**LLM Memory** = Conversation history across multiple turns (not covered in this notebook)\n",
    "\n",
    "In a complete RAG chatbot, your context window must fit:\n",
    "- System prompt + Retrieved chunks (this notebook) + Conversation history (memory) + Current query + Response space\n",
    "\n",
    "Context window management ensures retrieved chunks fit. Memory management (conversation history) would be an additional concern for multi-turn chatbots.\n",
    "\n",
    "### The Problem:\n",
    "Every LLM has a **context window** (maximum token limit). For example:\n",
    "- GPT-3.5-turbo: ~4K tokens\n",
    "- GPT-4: ~8K-128K tokens (depending on version)\n",
    "- Llama-3.1-8b: ~8K tokens\n",
    "\n",
    "When you retrieve many chunks or have long documents, the combined context can exceed this limit, causing:\n",
    "- API errors (request rejected)\n",
    "- Truncated context (LLM only sees partial information)\n",
    "- Increased costs (more tokens = higher price)\n",
    "\n",
    "### Real-World Challenge:\n",
    "**What if retrieved context exceeds the LLM's token limit?**\n",
    "\n",
    "You need intelligent strategies to fit context within budget while preserving the most important information.\n",
    "\n",
    "### Available Strategies:\n",
    "\n",
    "1. **Truncation (Simple & Fast)**\n",
    "   - Keep only top-N highest-scoring chunks\n",
    "   - Stops adding chunks when token limit reached\n",
    "   - ‚úÖ Preserves most relevant information\n",
    "   - ‚ùå May lose valuable context from lower-ranked chunks\n",
    "\n",
    "2. **Summarization (Intelligent Compression)**\n",
    "   - Use another LLM call to compress context\n",
    "   - Condenses multiple chunks into key points\n",
    "   - ‚úÖ Maintains semantic meaning in less space\n",
    "   - ‚ùå Adds latency and cost (extra LLM call)\n",
    "   - ‚ùå May lose specific details or nuances\n",
    "\n",
    "3. **Chunked Processing (Divide & Conquer)**\n",
    "   - Process chunks in batches, generate partial answers\n",
    "   - Merge partial answers into final response\n",
    "   - ‚úÖ Handles very large contexts\n",
    "   - ‚ùå Complex implementation, multiple LLM calls\n",
    "\n",
    "4. **Smart Filtering (Preprocessing)**\n",
    "   - Remove redundant sentences using similarity\n",
    "   - Filter out low-information content\n",
    "   - ‚úÖ Reduces noise without LLM calls\n",
    "   - ‚ùå Requires additional processing\n",
    "\n",
    "**In this demo, we'll implement and compare strategies #1 (Truncation) and #2 (Summarization).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27987731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TOKEN ESTIMATION & CONTEXT WINDOW MANAGEMENT\n",
    "# ============================================================================\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"\n",
    "    Estimate the number of tokens in text.\n",
    "    \n",
    "    This is a rough approximation. Real tokenization depends on the specific\n",
    "    tokenizer used by the LLM (e.g., tiktoken for OpenAI models).\n",
    "    \n",
    "    Rule of thumb for English:\n",
    "    - 1 token ‚âà 4 characters\n",
    "    - 1 token ‚âà 0.75 words\n",
    "    - 100 tokens ‚âà 75 words\n",
    "    \n",
    "    For production use, consider using actual tokenizers:\n",
    "    - OpenAI: tiktoken library\n",
    "    - Hugging Face: transformers.AutoTokenizer\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to estimate tokens for\n",
    "        \n",
    "    Returns:\n",
    "        Estimated token count (int)\n",
    "    \"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def manage_context_window(\n",
    "    retrieved_results,\n",
    "    max_tokens=2000,\n",
    "    strategy=\"truncate\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Intelligently manage context to fit within LLM token budget.\n",
    "    \n",
    "    This function implements two strategies for handling contexts that may\n",
    "    exceed the LLM's maximum token limit:\n",
    "    \n",
    "    1. TRUNCATION: Greedily add chunks (sorted by relevance) until budget exhausted\n",
    "       - Fast and simple\n",
    "       - Preserves most relevant chunks\n",
    "       - No additional LLM calls\n",
    "       \n",
    "    2. SUMMARIZATION: Use LLM to compress context into shorter form\n",
    "       - Better semantic preservation\n",
    "       - Requires extra LLM call (adds latency + cost)\n",
    "       - May lose specific details\n",
    "    \n",
    "    Args:\n",
    "        retrieved_results: List of dicts with 'content', 'chunk_idx', \n",
    "                          'hybrid_score', and 'metadata' keys\n",
    "        max_tokens: Maximum tokens allowed for context (reserve space for \n",
    "                   prompt template and LLM response)\n",
    "        strategy: 'truncate' or 'summarize'\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dicts that fit within token budget\n",
    "        \n",
    "    Example:\n",
    "        >>> chunks = hybrid_retrieval(\"query\", top_k=10)\n",
    "        >>> managed = manage_context_window(chunks, max_tokens=2000, strategy=\"truncate\")\n",
    "        >>> # managed now contains only chunks that fit in 2000 tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    if strategy == \"truncate\":\n",
    "        # STRATEGY 1: TRUNCATION\n",
    "        # Iterate through chunks (already sorted by relevance) and add them\n",
    "        # until we hit the token budget limit\n",
    "        \n",
    "        selected = []  # Chunks that fit within budget\n",
    "        current_tokens = 0  # Running token count\n",
    "        \n",
    "        for result in retrieved_results:\n",
    "            # Estimate tokens for this chunk\n",
    "            chunk_tokens = estimate_tokens(result['content'])\n",
    "            \n",
    "            # Check if adding this chunk would exceed budget\n",
    "            if current_tokens + chunk_tokens <= max_tokens:\n",
    "                selected.append(result)\n",
    "                current_tokens += chunk_tokens\n",
    "            else:\n",
    "                # Budget exhausted, stop adding chunks\n",
    "                # Note: This means lower-ranked chunks are discarded\n",
    "                break\n",
    "        \n",
    "        print(f\"‚úÇÔ∏è Truncated to {len(selected)}/{len(retrieved_results)} chunks\")\n",
    "        print(f\"üìä Estimated tokens: {current_tokens}/{max_tokens}\")\n",
    "        print(f\"üí∞ Token savings: {estimate_tokens(''.join([r['content'] for r in retrieved_results])) - current_tokens} tokens\")\n",
    "        return selected\n",
    "    \n",
    "    elif strategy == \"summarize\":\n",
    "        # STRATEGY 2: SUMMARIZATION\n",
    "        # First check if context exceeds budget, then use LLM to compress\n",
    "        \n",
    "        # Combine all chunks into single text\n",
    "        full_text = \"\\n\\n\".join([r['content'] for r in retrieved_results])\n",
    "        total_tokens = estimate_tokens(full_text)\n",
    "        \n",
    "        if total_tokens > max_tokens:\n",
    "            # Context too large - need to summarize\n",
    "            print(f\"üìâ Context too large ({total_tokens} tokens > {max_tokens} limit)\")\n",
    "            print(f\"ü§ñ Calling LLM to summarize...\")\n",
    "            \n",
    "            # Use the summarized_context function (defined earlier) to compress\n",
    "            # This makes an LLM call to condense the context\n",
    "            summary = summarized_context(retrieved_results, groq_client)\n",
    "            summary_tokens = estimate_tokens(summary)\n",
    "            \n",
    "            print(f\"‚úÖ Summarized to {summary_tokens} tokens (reduction: {total_tokens - summary_tokens} tokens)\")\n",
    "            \n",
    "            # Return as a pseudo-chunk with special metadata\n",
    "            return [{\n",
    "                'chunk_idx': -1,  # Special ID indicating this is a summary\n",
    "                'content': summary,\n",
    "                'hybrid_score': 1.0,  # Max score since it's a summary of all\n",
    "                'metadata': {\n",
    "                    'source': 'Summary',\n",
    "                    'topic': 'Consolidated',\n",
    "                    'original_chunks': len(retrieved_results),\n",
    "                    'original_tokens': total_tokens\n",
    "                }\n",
    "            }]\n",
    "        else:\n",
    "            # Context already fits within budget - no action needed\n",
    "            print(f\"‚úÖ Context within budget ({total_tokens}/{max_tokens} tokens)\")\n",
    "            return retrieved_results\n",
    "    \n",
    "    # Fallback: return original results if strategy not recognized\n",
    "    return retrieved_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DEMONSTRATION: Context Window Management\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONTEXT WINDOW MANAGEMENT DEMO\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"üí° Scenario: We retrieve 5 chunks, but they might exceed our token budget\")\n",
    "print(\"   Let's see how truncation and summarization handle this...\\n\")\n",
    "\n",
    "# Retrieve more chunks than usual to demonstrate the problem\n",
    "large_retrieval = hybrid_retrieval(test_query, top_k=5)\n",
    "\n",
    "# Show original context size\n",
    "original_text = \"\\n\\n\".join([r['content'] for r in large_retrieval])\n",
    "original_tokens = estimate_tokens(original_text)\n",
    "print(f\"üìè Original retrieval: {len(large_retrieval)} chunks, ~{original_tokens} tokens\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# STRATEGY 1: TRUNCATION\n",
    "# -------------------------\n",
    "print(\"=\" * 80)\n",
    "print(\"Strategy 1: Truncation (Keep top chunks until budget exhausted)\")\n",
    "print(\"=\" * 80)\n",
    "managed_truncate = manage_context_window(large_retrieval, max_tokens=500, strategy=\"truncate\")\n",
    "print(f\"\\nüëâ Result: Kept {len(managed_truncate)} most relevant chunks\")\n",
    "print(\"   Use case: When you want to preserve exact wording of top chunks\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# STRATEGY 2: SUMMARIZATION\n",
    "# -------------------------\n",
    "print(\"=\" * 80)\n",
    "print(\"Strategy 2: Summarization (Compress context using LLM)\")\n",
    "print(\"=\" * 80)\n",
    "managed_summarize = manage_context_window(large_retrieval, max_tokens=500, strategy=\"summarize\")\n",
    "print(f\"\\nüëâ Result: {len(managed_summarize)} chunk(s) - {'Summary' if managed_summarize[0]['chunk_idx'] == -1 else 'Original'}\")\n",
    "print(\"   Use case: When you need to fit more information in less space\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ Key Takeaway:\")\n",
    "print(\"   - Truncation: Fast, preserves details, but discards lower-ranked chunks\")\n",
    "print(\"   - Summarization: Compresses all info, but adds latency and may lose details\")\n",
    "print(\"   - Choose based on: token budget, latency requirements, and detail importance\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a0304",
   "metadata": {},
   "source": [
    "## 6. Comparison: Different Augmentation Approaches\n",
    "\n",
    "Let's compare how different augmentation strategies affect the final answer quality. We'll briefly use generation here for comparison purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you see duplicate outputs, clear the cell output and re-run once\n",
    "# Helper function for generation (used only for comparison)\n",
    "def generate_answer(augmented_result, model=\"llama-3.1-8b-instant\", temperature=0.3):\n",
    "    \"\"\"Generate answer using LLM with augmented prompt.\"\"\"\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": augmented_result['prompt']}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARATIVE ANALYSIS: AUGMENTATION STRATEGIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the internet speed requirements for remote work?\",\n",
    "    \"How is remote work performance evaluated?\",\n",
    "    \"What communication tools should remote workers use?\"\n",
    "]\n",
    "\n",
    "strategies = [\n",
    "    (\"simple\", \"basic\", \"Simple + Basic\"),\n",
    "    (\"numbered\", \"citation\", \"Numbered + Citation\"),\n",
    "    (\"metadata\", \"instructional\", \"Metadata + Instructional\"),\n",
    "]\n",
    "\n",
    "# Test first query with all strategies\n",
    "query = test_queries[0]\n",
    "print(f\"\\nüîç Query: '{query}'\\n\")\n",
    "\n",
    "# Clear any previous results (ensure clean state)\n",
    "if 'results_comparison' in dir() and len(results_comparison) > 0:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Found {len(results_comparison)} existing results. Clearing...\")\n",
    "results_comparison = []\n",
    "\n",
    "for context_strat, template_type, label in strategies:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {label}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Augment\n",
    "    augmented = augment_query(\n",
    "        query=query,\n",
    "        top_k=3,\n",
    "        context_strategy=context_strat,\n",
    "        template_type=template_type\n",
    "    )\n",
    "    \n",
    "    # Generate\n",
    "    print(\"ü§ñ Generating answer...\")\n",
    "    answer = generate_answer(augmented, temperature=0.2)  # Low temp for consistency\n",
    "    \n",
    "    # Calculate token metrics (more meaningful than character count)\n",
    "    prompt_tokens = estimate_tokens(augmented['prompt'])\n",
    "    answer_tokens = estimate_tokens(answer)\n",
    "    total_tokens = prompt_tokens + answer_tokens  # Total tokens sent + received\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'strategy': label,\n",
    "        'answer': answer,\n",
    "        'prompt_tokens': prompt_tokens,\n",
    "        'answer_tokens': answer_tokens,\n",
    "        'total_tokens': total_tokens\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìù Answer ({answer_tokens} tokens):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(answer)\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\nüìä Token Usage: {prompt_tokens} input + {answer_tokens} output = {total_tokens} total tokens\")\n",
    "\n",
    "# Combined token usage and cost comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOKEN USAGE & COST COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nUsing GPT-4 pricing ($0.03/1K input, $0.06/1K output):\")\n",
    "print(f\"\\n{'Strategy':<30} {'Input':>10} {'Output':>10} {'Total':>10} {'Cost':>12}\")\n",
    "print(f\"{'':30} {'(tokens)':>10} {'(tokens)':>10} {'(tokens)':>10} {'($)':>12}\")\n",
    "print(\"-\" * 80)\n",
    "for r in results_comparison:\n",
    "    input_cost = (r['prompt_tokens'] / 1000) * 0.03\n",
    "    output_cost = (r['answer_tokens'] / 1000) * 0.06\n",
    "    total_cost = input_cost + output_cost\n",
    "    print(f\"{r['strategy']:<30} {r['prompt_tokens']:>10} {r['answer_tokens']:>10} {r['total_tokens']:>10} ${total_cost:>11.4f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate and show averages\n",
    "avg_prompt = sum(r['prompt_tokens'] for r in results_comparison) // len(results_comparison)\n",
    "avg_answer = sum(r['answer_tokens'] for r in results_comparison) // len(results_comparison)\n",
    "avg_total = sum(r['total_tokens'] for r in results_comparison) // len(results_comparison)\n",
    "avg_cost = sum((r['prompt_tokens'] / 1000) * 0.03 + (r['answer_tokens'] / 1000) * 0.06 for r in results_comparison) / len(results_comparison)\n",
    "print(f\"{'AVERAGE':<30} {avg_prompt:>10} {avg_answer:>10} {avg_total:>10} ${avg_cost:>11.4f}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Show cost at scale\n",
    "print(\"\\nüí∞ Cost at Scale (GPT-4):\")\n",
    "print(f\"   - 1,000 queries/day: ${avg_cost * 1000:.2f}/day = ${avg_cost * 30000:.2f}/month\")\n",
    "print(f\"   - 10,000 queries/day: ${avg_cost * 10000:.2f}/day = ${avg_cost * 300000:.2f}/month\")\n",
    "print(\"\\nüí° Using cheaper models (GPT-3.5: $0.0015/$0.002) reduces costs by ~95%\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447778e",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook covered the **Augmentation** phase of RAG pipelines - how to combine retrieved chunks with user queries into effective LLM prompts.\n",
    "\n",
    "### What We Implemented:\n",
    "\n",
    "**1. Prompt Templates (4 types)**\n",
    "- Basic QA: Simple context + question format\n",
    "- Instructional: Role definition with clear constraints\n",
    "- Citation-focused: Emphasizes source attribution\n",
    "- Chain-of-Thought: Encourages step-by-step reasoning\n",
    "\n",
    "**2. Context Integration Strategies (5 approaches)**\n",
    "- Simple Concatenation: Fast, preserves all details\n",
    "- Numbered Context: Adds chunk labels for citations\n",
    "- Metadata-Enriched: Includes source/topic information\n",
    "- Hierarchical: Sorted by relevance scores\n",
    "- Summarized: LLM-compressed for token savings\n",
    "\n",
    "**3. Complete Augmentation Pipeline**\n",
    "- Combines retrieval ‚Üí context formatting ‚Üí prompt template\n",
    "- Modular design for easy strategy switching\n",
    "- Demonstrated with real examples\n",
    "\n",
    "**4. Context Window Management**\n",
    "- Token estimation (4 chars ‚âà 1 token)\n",
    "- Truncation strategy: Keep top chunks until budget exhausted\n",
    "- Summarization strategy: LLM compression for better semantic preservation\n",
    "- Trade-offs: Speed vs detail preservation vs cost\n",
    "\n",
    "**5. Strategy Comparison**\n",
    "- Tested 3 augmentation approaches with actual LLM calls\n",
    "- Token usage analysis: Input, output, and total tokens\n",
    "- Cost analysis: Per-query and at-scale estimates (GPT-4 pricing)\n",
    "- Key insight: Structured prompts cost more tokens but improve answer quality\n",
    "\n",
    "### Key Concepts:\n",
    "- **Context Window**: Maximum tokens LLM can process (different from conversation memory)\n",
    "- **Token Budget**: Balance between context richness and API costs\n",
    "- **Citation Tracking**: Numbered chunks enable answer verification and trust\n",
    "- **Cost-Quality Tradeoff**: More structured context = higher cost but better results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
