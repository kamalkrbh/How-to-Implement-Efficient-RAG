{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports and test data (reused from earlier notebooks)\n",
    "from sample_data import SAMPLE_TEXT\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from groq import Groq  # replace with your LLM client if needed\n",
    "import time\n",
    "print('‚úÖ Setup imports complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4eb758",
   "metadata": {},
   "source": [
    "## 1) Setup & Reusable Helpers ‚Äî from previous parts\n",
    "\n",
    "Re-creating minimal helpers (chunking, embeddings, Chroma vector store, hybrid retrieval) so this notebook runs standalone. We'll also set up prompt templates and token estimation. Ensure `GROQ_API_KEY` is set in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0bf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- embeddings + vector store (minimal demo) ---\n",
    "print('üîÑ Loading embedding model...')\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print('‚úÖ Embedding model ready')\n",
    "# Chunk the sample document (simple sentence-based chunker reused)\n",
    "def chunk_by_semantic_similarity(text: str, similarity_threshold: float = 0.5, overlap_sentences: int = 2, min_chunk_size: int = 2) -> list:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= min_chunk_size:\n",
    "        return [text]\n",
    "    vec = TfidfVectorizer(stop_words='english')\n",
    "    try:\n",
    "        sentence_vectors = vec.fit_transform(sentences)\n",
    "    except ValueError:\n",
    "        return [text]\n",
    "    similarities = [cosine_similarity(sentence_vectors[i:i+1], sentence_vectors[i+1:i+2])[0][0] for i in range(len(sentences)-1)]\n",
    "    chunk_boundaries = [0]\n",
    "    current_chunk_size = 1\n",
    "    for i, sim in enumerate(similarities):\n",
    "        if sim < similarity_threshold and current_chunk_size >= min_chunk_size:\n",
    "            chunk_boundaries.append(i+1); current_chunk_size = 1\n",
    "        else:\n",
    "            current_chunk_size += 1\n",
    "    if chunk_boundaries[-1] != len(sentences):\n",
    "        chunk_boundaries.append(len(sentences))\n",
    "    chunks = []\n",
    "    for i in range(len(chunk_boundaries)-1):\n",
    "        start_idx = chunk_boundaries[i]\n",
    "        end_idx = chunk_boundaries[i+1]\n",
    "        if i > 0 and overlap_sentences > 0:\n",
    "            overlap_start = max(0, start_idx-overlap_sentences)\n",
    "            chunk_sentences = sentences[overlap_start:end_idx]\n",
    "        else:\n",
    "            chunk_sentences = sentences[start_idx:end_idx]\n",
    "        chunk = ' '.join(chunk_sentences)\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "chunks = chunk_by_semantic_similarity(SAMPLE_TEXT, similarity_threshold=0.15, overlap_sentences=2)\n",
    "print(f'‚úÖ Created {len(chunks)} chunks from sample data')\n",
    "# Build embeddings + Chroma collection (lightweight demo)\n",
    "embeddings = embedding_model.encode(chunks, show_progress_bar=False)\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(name='part5_generation_chunks', metadata={'hnsw:space':'cosine'})\n",
    "ids = [f'chunk_{i}' for i in range(len(chunks))]\n",
    "collection.add(ids=ids, embeddings=embeddings.tolist(), documents=chunks, metadatas=[{'chunk_index':i,'source':'SAMPLE_TEXT','length':len(chunks[i])} for i in range(len(chunks))])\n",
    "print(f'‚úÖ Vector store ready with {collection.count()} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0674ee16",
   "metadata": {},
   "source": [
    "### 1.1 Hybrid retrieval (dense + sparse) ‚Äî reused from earlier parts\n",
    "\n",
    "Balances BM25 (lexical) and semantic similarity for robust multi-faceted retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrieval(query: str, top_k: int = 3, alpha: float = 0.7):\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    qresults = collection.query(query_embeddings=[query_embedding.tolist()], n_results=top_k)\n",
    "    vector_scores_dict = {}\n",
    "    for chunk_id, distance in zip(qresults['ids'][0], qresults['distances'][0]):\n",
    "        chunk_idx = int(chunk_id.split('_')[1])\n",
    "        vector_scores_dict[chunk_idx] = 1 - distance\n",
    "    tokenized_chunks = [c.lower().split() for c in chunks]\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "    query_tokens = query.lower().split()\n",
    "    bm25_scores_dict = {idx: bm25.get_scores(query_tokens)[idx] for idx in vector_scores_dict.keys()}\n",
    "    bm25_list = list(bm25_scores_dict.values())\n",
    "    bm25_min, bm25_max = min(bm25_list), max(bm25_list)\n",
    "    bm25_norm = {k: (v - bm25_min) / (bm25_max - bm25_min + 1e-10) for k, v in bm25_scores_dict.items()}\n",
    "    hybrid_scores = {k: alpha * vector_scores_dict[k] + (1-alpha)*bm25_norm[k] for k in vector_scores_dict.keys()}\n",
    "    sorted_results = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    results = []\n",
    "    for idx, score in sorted_results:\n",
    "        meta = collection.get(ids=[f'chunk_{idx}'])['metadatas'][0]\n",
    "        results.append({'chunk_idx': idx, 'hybrid_score': score, 'vector_score': vector_scores_dict[idx], 'bm25_score': bm25_norm[idx], 'content': chunks[idx], 'metadata': meta})\n",
    "    return results\n",
    "print('‚úÖ Hybrid retrieval helper ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295d30c",
   "metadata": {},
   "source": [
    "### 1.2 Prompt Templates & LLM Generation\n",
    "\n",
    "Defines multiple prompt templates (QA, instructional, citations, email, chain-of-thought) and a `generate_answer()` helper that calls Groq and returns tokens/timing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates tailored for generation & attribution\n",
    "BASIC_QA_TEMPLATE = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print('‚úÖ Generation templates ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1eadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token estimation util (reuse the same heuristic used earlier)\n",
    "def estimate_tokens(text: str):\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "# LLM client (Groq used in earlier notebooks) ‚Äî ensure API key is set in the environment\n",
    "groq_client = Groq(api_key=os.environ.get('GROQ_API_KEY')) if os.environ.get('GROQ_API_KEY') else None\n",
    "if groq_client:\n",
    "    print('‚úÖ LLM client ready (GROQ)')\n",
    "else:\n",
    "    print('‚ö†Ô∏è GROQ_API_KEY missing ‚Äî generation calls will be mocked unless you set the environment variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950d266",
   "metadata": {},
   "source": [
    "#### 1.2.1 generate_answer helper\n",
    "\n",
    "Wraps the Groq LLM call with timing and token metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt_text: str, model='llama-3.1-8b-instant', temperature=0.2, max_output_tokens=400, \n",
    "                   top_p=1.0, top_k=None, frequency_penalty=0.0, presence_penalty=0.0, stop=None):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Build API parameters dynamically\n",
    "    api_params = {\n",
    "        'model': model,\n",
    "        'messages': [{'role':'user','content': prompt_text}],\n",
    "        'temperature': temperature,\n",
    "        'max_tokens': max_output_tokens,\n",
    "        'top_p': top_p,\n",
    "        'frequency_penalty': frequency_penalty,\n",
    "        'presence_penalty': presence_penalty,\n",
    "    }\n",
    "    \n",
    "    if top_k is not None:\n",
    "        api_params['top_k'] = top_k\n",
    "    \n",
    "    if stop is not None:\n",
    "        api_params['stop'] = stop\n",
    "    \n",
    "    response = groq_client.chat.completions.create(**api_params)\n",
    "    out = response.choices[0].message.content.strip()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return {'text': out, 'tokens': estimate_tokens(out), 'elapsed': elapsed, 'raw': response}\n",
    "\n",
    "print('‚úÖ generate_answer helper ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a72c13",
   "metadata": {},
   "source": [
    "## 2) LLM Decoding Parameters: Complete Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129952e",
   "metadata": {},
   "source": [
    "### 2.1 **TEMPERATURE** (0.0 to 2.0+) | Default: 1.0\n",
    "Controls randomness/creativity of next token selection.\n",
    "\n",
    "**Low (0.0-0.3):**\n",
    "- ‚úì Deterministic and focused\n",
    "- ‚úì Best for factual Q&A, code generation\n",
    "- ‚úì Repeatable outputs (ideal for testing)\n",
    "- ‚úó May be boring or repetitive\n",
    "- Example: `temperature=0.1` for grounded RAG answers\n",
    "\n",
    "**Medium (0.7-1.0):**\n",
    "- ‚úì Balanced: coherent + somewhat creative\n",
    "- ‚úì Good default for most tasks\n",
    "- ‚úì Natural-sounding responses\n",
    "- Example: `temperature=0.7` for general QA\n",
    "\n",
    "**High (1.2+):**\n",
    "- ‚úì Very creative, explores more diverse tokens\n",
    "- ‚úó Increased hallucination risk\n",
    "- ‚úó May become incoherent\n",
    "- ‚ö†Ô∏è Use sparingly for brainstorming only\n",
    "- Example: `temperature=1.5` for creative writing\n",
    "\n",
    "üß† **Effects on LLM Output:**\n",
    "- Changes how sharp or flat the probability distribution is\n",
    "- Low ‚Üí deterministic, safe, repetitive\n",
    "- High ‚Üí creative, varied, risky\n",
    "- Affects style, not length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a558eee",
   "metadata": {},
   "source": [
    "#### üî¨ Demo: Temperature in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0384d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: How temperature affects output creativity\n",
    "test_query = \"What are the main requirements of remote work for employees and the organization? What are the benefits ?\"\n",
    "ctx = '\\n\\n'.join([f\"[chunk_{r['chunk_idx']}]\\n{r['content']}\" for r in hybrid_retrieval(test_query, top_k=2)])\n",
    "prompt = BASIC_QA_TEMPLATE.format(context=ctx, query=test_query)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üî¨ TEMPERATURE DEMO: Same prompt, different temperature values\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for temp_value in [0.01, 1.0, 2.0]:\n",
    "    print(f\"\\n{'‚îÄ' * 80}\")\n",
    "    print(f\"üå°Ô∏è Temperature = {temp_value}\")\n",
    "    print(f\"{'‚îÄ' * 80}\")\n",
    "    \n",
    "    result = generate_answer(\n",
    "        prompt_text=prompt, \n",
    "        #prompt_text=\"What does AI mean for a 10 year old?\",\n",
    "        temperature=temp_value,\n",
    "        max_output_tokens=500\n",
    "    )\n",
    "    \n",
    "    print(f\"Output: {result['text']}.\")\n",
    "    print(f\"Tokens: {result['tokens']} | Time: {result['elapsed']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048158e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 **TOP_P (NUCLEUS SAMPLING)** (0.0 to 1.0) | Default: 1.0 (disabled)\n",
    "Keep the smallest set of tokens whose cumulative probability reaches or exceeds `top_p`; discard the rest.\n",
    "\n",
    "**How it works:**\n",
    "1. LLM ranks next tokens by probability: [0.4, 0.3, 0.15, 0.1, 0.05]\n",
    "2. Accumulate until cumsum ‚â• top_p. With top_p=0.9: 0.4+0.3+0.15=0.85 (<0.9) ‚Üí add 0.1 ‚Üí 0.95 (stop)\n",
    "3. Tokens after the cutoff (0.05 here) are excluded; the set always crosses the threshold, never stops below it\n",
    "4. Result: More coherent, fewer low-probability odd tokens\n",
    "\n",
    "**Example with words:**\n",
    "- Sentence prefix: \"Today I went to the\"\n",
    "- Candidate next words with probs: store(0.46), park(0.22), office(0.12), beach(0.09), moon(0.06), volcano(0.05)\n",
    "- `top_p=0.8` nucleus: store + park + office = 0.80 (LLM samples only from these 3)\n",
    "- `top_p=0.9` nucleus: store + park + office + beach + moon = 0.95 (needs to cross 0.9; moon is added)\n",
    "- `top_p=1.0`: keeps all options, including moon/volcano (most diverse)\n",
    "\n",
    "**Recommended values:**\n",
    "- `top_p=0.9`: Recommended; filters out very unlikely tokens\n",
    "- `top_p=1.0`: Disabled (use all tokens)\n",
    "- `top_p=0.5`: Very restrictive; safer but may limit diversity\n",
    "\n",
    "üß† **Effects on LLM Output:**\n",
    "- Limits which tokens are allowed based on probability mass\n",
    "- Low ‚Üí only very likely tokens\n",
    "- High ‚Üí more linguistic variety\n",
    "- Improves coherence vs. creativity balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7159ef",
   "metadata": {},
   "source": [
    "#### 2.2.1 Temperature vs. top_p ‚Äî concise and practical\n",
    "\n",
    "\n",
    "#### üß™ Concrete example\n",
    "\n",
    "##### Original probabilities\n",
    "```\n",
    "A: 0.50\n",
    "B: 0.30\n",
    "C: 0.15\n",
    "D: 0.05\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### Case 1: Low temperature (0.2)\n",
    "```\n",
    "A: 0.85\n",
    "B: 0.10\n",
    "C: 0.04\n",
    "D: 0.01\n",
    "```\n",
    "Now with `top_p = 0.9`:\n",
    "\n",
    "* Allowed: A + B\n",
    "* Very deterministic\n",
    "\n",
    "---\n",
    "\n",
    "##### Case 2: High temperature (1.2)\n",
    "```\n",
    "A: 0.35\n",
    "B: 0.30\n",
    "C: 0.20\n",
    "D: 0.15\n",
    "```\n",
    "Now with `top_p = 0.9`:\n",
    "\n",
    "* Allowed: A + B + C + D\n",
    "* Much more variation\n",
    "\n",
    "üëâ Same top_p, different result\n",
    "Because temperature changed the distribution before top_p was applied.\n",
    "\n",
    "Practical defaults:\n",
    "- For RAG stability: top_p ‚âà 0.9, temperature ‚âà 0.2‚Äì0.4.\n",
    "- For more creativity: increase temperature; adjust top_p cautiously to avoid drift/hallucinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1720acc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3 **TOP_K** (1 to vocab_size, typically 50-100) | Default: Not set (disabled)\n",
    "Keep only top-k highest probability tokens; ignore all others.\n",
    "\n",
    "**How it works:**\n",
    "1. LLM ranks tokens by probability: [0.4, 0.3, 0.15, 0.1, 0.05]\n",
    "2. top_k=3 keeps only: [0.4, 0.3, 0.15]\n",
    "3. Discards low-probability tokens entirely\n",
    "4. Result: Simpler but less flexible than top_p\n",
    "\n",
    "**Example with words:**\n",
    "- Sentence prefix: \"Today I went to the\"\n",
    "- Candidate next words with probs: store(0.46), park(0.22), office(0.12), beach(0.09), moon(0.06), volcano(0.05)\n",
    "- `top_k=3`: keeps only store, park, office (exactly top 3 by probability)\n",
    "- `top_k=5`: keeps store, park, office, beach, moon (top 5; excludes volcano)\n",
    "- `top_k=6`: keeps all options (no restriction; moon and volcano both allowed)\n",
    "\n",
    "**Recommended values:**\n",
    "- `top_k=50`: Common; balances diversity and safety\n",
    "- `top_k=10`: Very restrictive; deterministic\n",
    "- `top_k=100`: More permissive; allows more variety\n",
    "\n",
    "**Note:** Less popular than top_p; top_p is generally preferred for nuanced control\n",
    "\n",
    "üß† **Effects on LLM Output:**\n",
    "- Limits the number of possible tokens\n",
    "- Low ‚Üí strict, predictable\n",
    "- High ‚Üí diverse, sometimes noisy\n",
    "- Less adaptive than top_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd5b8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.4 **MAX_TOKENS** (1 to model_max, e.g., 8192 for Llama 3.1-8b) | Default: Model-dependent\n",
    "Maximum length of the generated response (hard limit).\n",
    "\n",
    "**Impact:**\n",
    "- **Cost:** Longer max_tokens ‚Üí higher price (pay per output token)\n",
    "- **Latency:** Longer sequences take more time to generate\n",
    "- **Truncation:** If output hits limit, it may be cut off mid-sentence\n",
    "\n",
    "**Guidance:** Set to ~1.5√ó your expected answer length to avoid abrupt cutoff\n",
    "\n",
    "üß† **Effects on LLM Output:**\n",
    "- Limits response length\n",
    "- Controls cost and latency\n",
    "- Does not change creativity or meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0cb56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.5 **FREQUENCY_PENALTY** (0.0 to 2.0) | Default: 0.0 (disabled)\n",
    "Reduces the probability of tokens that have already appeared in the generated text.\n",
    "In simple terms: ‚ÄúDon‚Äôt keep repeating the same words.‚Äù\n",
    "\n",
    "- Higher values reduce repetition.\n",
    "\n",
    "**Example:**\n",
    "- Generated so far: \"The research shows that the method is effective. The results indicate that the\"\n",
    "- `frequency_penalty=0.0`: word \"the\" has same probability as usual\n",
    "- `frequency_penalty=0.5`: word \"the\" gets penalty; LLM tries alternatives like \"results\" or \"findings\"\n",
    "- `frequency_penalty=1.0`: strong penalty on \"the\"; LLM strongly avoids this word (avoids repetition)\n",
    "\n",
    "**Use case:** `frequency_penalty=0.5` to `1.0` reduces repetition in longer outputs\n",
    "\n",
    "üß† **Effects on LLM Output:**\n",
    "- Reduces repetition of frequently used tokens\n",
    "- Encourages lexical diversity\n",
    "- Too high ‚Üí unnatural wording"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f402c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.6 **PRESENCE_PENALTY** (0.0 to 2.0) | Default: 0.0 (disabled)\n",
    "Binary penalty: if a token appears 1 or more times in output, apply penalty (regardless of frequency).\n",
    "\n",
    "**How it works:**\n",
    "1. Tracks which tokens have appeared at least once\n",
    "2. If token reappears: penalty applies equally (not scaled by count)\n",
    "3. Useful for avoiding specific repeated phrases/tokens\n",
    "4. Different from frequency_penalty which scales by occurrence count\n",
    "\n",
    "**Example:**\n",
    "- Generated so far: \"In conclusion, I believe this approach is sound.\"\n",
    "- `presence_penalty=0.0`: \"conclusion\" can reappear with normal probability\n",
    "- `presence_penalty=0.6`: \"conclusion\" gets penalized if used again (penalty same whether 1 or 5 times)\n",
    "- `presence_penalty=1.0`: strong penalty; \"In conclusion\" phrase likely avoided entirely\n",
    "\n",
    "**Recommended values:**\n",
    "- `presence_penalty=0.0`: Disabled; no penalty for reusing tokens\n",
    "- `presence_penalty=0.6`: Moderate penalty; discourages repeating any phrase\n",
    "- `presence_penalty=1.0+`: Strong penalty; strongly avoids reused tokens\n",
    "\n",
    "**Use case:** `presence_penalty=0.6` avoids repeating phrases like \"in conclusion\" or \"furthermore\" multiple times\n",
    "\n",
    "üß† **Effects on LLM Output:**\n",
    "- Discourages reuse of already used concepts\n",
    "- Pushes model toward new ideas/topics\n",
    "- Can cause topic drift if high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1ad16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 2.6.1 Frequency Penalty vs. Presence Penalty ‚Äî Key Differences\n",
    "\n",
    "üß† **The key difference **\n",
    "\n",
    "In frequency panelty more occurence of word will increase the penalty but in presence penalty once the word is used the constant penalty will be applied no matter how many times it is used again.\n",
    "\n",
    "- **Frequency penalty** says: \"You're using this too often ‚Äî slow down.\"\n",
    "- **Presence penalty** says: \"You already used this ‚Äî try something new.\"\n",
    "---\n",
    "\n",
    "üß™ **Simple side-by-side example, Topic Shift**\n",
    "\n",
    "**Prompt:** \"Explain DNS briefly.\"\n",
    "\n",
    "**No penalties:**\n",
    "\n",
    "> DNS resolves domain names into IP addresses. \n",
    "> DNS helps clients locate servers. \n",
    "> DNS also caches addresses for faster lookup. \n",
    "\n",
    "**Frequency penalty only:**\n",
    "\n",
    "> DNS resolves domain names into IP addresses. \n",
    "> This system helps clients locate servers.\n",
    "> DNS also caches addresses for faster lookup.\n",
    "\n",
    "> ‚û° Same topic, better wording\n",
    "\n",
    "**Presence penalty only:**\n",
    "\n",
    "> DNS resolves domain names into IP addresses.\n",
    "> This system enables clients to find servers efficiently.\n",
    "> Additionally, it maintains a cache to speed up requests.\n",
    "\n",
    "> ‚Üí Notice: The model uses new words and phrases, slightly shifting focus to efficiency, caching, and functionality. This can feel like a subtle topic expansion or shift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e615c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.7 **STOP** (List of strings or None) | Default: None (disabled)\n",
    "Stop sequences: generation halts when any of these strings is produced.\n",
    "\n",
    "**How it works:**\n",
    "1. Specify one or more stop sequences (e.g., `[\"\\n\\n\", \"User:\", \"###\"]`)\n",
    "2. LLM generates tokens until one of these sequences is produced\n",
    "3. The stop sequence itself is NOT included in the output\n",
    "4. Useful for controlling output structure and preventing over-generation\n",
    "\n",
    "**Example:**\n",
    "- Prompt: \"Write a short bio for Jane. Stop when done.\"\n",
    "- `stop=None`: LLM keeps generating (may write extra paragraphs)\n",
    "- `stop=[\"\\n\\n\"]`: Output = \"Jane is a developer with 10 years of experience.\" (stops at double newline)\n",
    "- `stop=[\"Sources:\", \"References:\"]`: For Q&A, stops before LLM tries to fabricate citations\n",
    "\n",
    "**Recommended values:**\n",
    "- `stop=None`: Disabled; generation continues until max_tokens reached\n",
    "- `stop=[\"\\n\\n\"]`: Common; stops after double newline (paragraph break)\n",
    "- `stop=[\"User:\", \"Assistant:\"]`: Useful for chat turn-taking; stops at next speaker label\n",
    "- `stop=[\"###\", \"---\"]`: Good for structured outputs; stops at section break\n",
    "\n",
    "**Use case:** `stop=[\"\\n\\n\"]` prevents multi-paragraph answers when single-paragraph response is desired; RAG: `stop=[\"Sources:\", \"References:\"]` to control citation placement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c89b8e",
   "metadata": {},
   "source": [
    "## 3) Practical Demo: Decoding Parameter Trade-offs\n",
    "\n",
    "Now that you understand each parameter, let's see how they affect token count, latency, and quality. We'll vary temperature and max_tokens across two model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1232b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_decoding_parameters(query: str):\n",
    "    \"\"\"Compare outputs and metrics across different parameter settings.\"\"\"\n",
    "    \n",
    "    # Retrieve context (reused for all attempts)\n",
    "    ctx = '\\n\\n'.join([f\"[chunk_{r['chunk_idx']}]\\n{r['content']}\" for r in hybrid_retrieval(query, top_k=2)])\n",
    "    base_prompt = BASIC_QA_TEMPLATE.format(context=ctx, query=query)\n",
    "    \n",
    "    # Test different parameter configurations\n",
    "    configs = [\n",
    "        {\n",
    "            'name': 'Conservative (Factual RAG)',\n",
    "            'temperature': 0.1,\n",
    "            'max_tokens': 200,\n",
    "            'top_p': 0.9,\n",
    "            'frequency_penalty': 0.5,\n",
    "            'stop': None,\n",
    "            'desc': 'Deterministic; best for Q&A grounded in context'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Balanced (Natural Conversation)',\n",
    "            'temperature': 0.7,\n",
    "            'max_tokens': 300,\n",
    "            'top_p': 0.95,\n",
    "            'frequency_penalty': 0.0,\n",
    "            'stop': None,\n",
    "            'desc': 'Moderate creativity; natural but still coherent'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Creative (Brainstorming)',\n",
    "            'temperature': 1.5,\n",
    "            'max_tokens': 300,\n",
    "            'top_p': 1.0,\n",
    "            'frequency_penalty': 0.2,\n",
    "            'stop': None,\n",
    "            'desc': 'High randomness; generates diverse ideas'\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    print('\\n' + '='*80)\n",
    "    print('DECODING PARAMETER COMPARISON DEMO')\n",
    "    print('='*80)\n",
    "    print(f'Query: \"{query}\"\\n')\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\nüìä {config['name']}\")\n",
    "        print(f\"   Settings: temp={config['temperature']}, max_tokens={config['max_tokens']}, top_p={config['top_p']}\")\n",
    "        print(f\"   Purpose: {config['desc']}\")\n",
    "        print('-' * 80)\n",
    "        \n",
    "        response = generate_answer(\n",
    "            base_prompt,\n",
    "            model='llama-3.1-8b-instant',\n",
    "            temperature=config['temperature'],\n",
    "            max_output_tokens=config['max_tokens'],\n",
    "            top_p=config.get('top_p', 1.0),\n",
    "            frequency_penalty=config.get('frequency_penalty', 0.0),\n",
    "            presence_penalty=config.get('presence_penalty', 0.0),\n",
    "            stop=config.get('stop', None)\n",
    "        )\n",
    "        \n",
    "        results[config['name']] = response\n",
    "        \n",
    "        print(f\"   Tokens: {response['tokens']} | Time: {response['elapsed']:.3f}s\")\n",
    "        print(f\"   Output (first 300 chars):\\n   {response['text'][:300]}...\")\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print('KEY INSIGHTS')\n",
    "    print('='*80)\n",
    "    print('''\n",
    "    1. Conservative (temp=0.1): Predictable, repetitive, great for grounded answers\n",
    "    2. Balanced (temp=0.7): Natural variation while staying on-topic\n",
    "    3. Creative (temp=1.5): More diverse ideas but higher hallucination risk\n",
    "    \n",
    "    ‚Üí For RAG: Use Conservative settings (temperature ‚â§ 0.3)\n",
    "    ‚Üí For conversation: Use Balanced settings (temperature ‚âà 0.7)\n",
    "    ‚Üí For creative tasks only: Use Creative (temperature > 1.0)\n",
    "    ''')\n",
    "    print('='*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "param_results = compare_decoding_parameters(test_query)\n",
    "\n",
    "# Run comparison demotest_query = 'What are the main eligibility criteria and performance standards for remote work at our organization?'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
